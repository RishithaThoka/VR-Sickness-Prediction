
import pandas as pd
import numpy as np
import os
from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, StackingClassifier
from sklearn.svm import LinearSVC
from xgboost import XGBClassifier
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, classification_report
from sklearn.calibration import CalibratedClassifierCV
from imblearn.over_sampling import SMOTE
import matplotlib.pyplot as plt
import seaborn as sns
import warnings
warnings.filterwarnings("ignore")


# Set random seed for reproducibility
np.random.seed(42)
demographics_data = pd.read_excel(demographics_file)

demographics_data.columns = [
    'Subject', 'Sex', 'Driving_Status', 'Age', 'Height_cm', 'Weight_lbs', 'Weight_kg',
    'Play_Video_Games', 'Age_Started_Games', 'Years_Played_Games', 'Hours_Per_Week_Games',
    'Used_VR_Headset', 'Owns_VR_Headset', 'Hours_Per_Week_VR', 'First_Visual_Task',
    'Second_Visual_Task', 'Discontinuation_Time_Secs'
]

demographics_data = demographics_data[[
    'Subject', 'Sex', 'Driving_Status', 'Age', 'Height_cm', 'Weight_lbs',
    'Play_Video_Games', 'Age_Started_Games', 'Years_Played_Games', 'Hours_Per_Week_Games',
    'Used_VR_Headset', 'Owns_VR_Headset', 'Hours_Per_Week_VR'
]]
# 1.2 Load SSQ Data
ssq_file = "/content/APAL 2019 SSQ Data.xlsx"
ssq_data = pd.read_excel(ssq_file)

ssq_data.columns = [
    'Subject', 'SSQ1_Tc', 'Indicated_Sick_SSQ1', 'SSQ2_Tc', 'Indicated_Sick_SSQ2',
    'Did_Participate_SSQ3', 'SSQ3_Tc', 'Indicated_Sick_SSQ3', 'Post_Exposure_Score'
]

ssq_data = ssq_data[['Subject', 'SSQ1_Tc', 'SSQ2_Tc', 'Indicated_Sick_SSQ2', 'Post_Exposure_Score']]

# 1.3 Load Polhemus Data
polhemus_female_file = "/content/PolhemusFemale.xlsx"
polhemus_male_file = "/content/PolhemusMale.xlsx"
polhemus_summaries = []

# List of subjects from demographics data
subjects = demographics_data['Subject'].tolist()

# Get available worksheets in Polhemus files
polhemus_female_xls = pd.ExcelFile(polhemus_female_file)
polhemus_male_xls = pd.ExcelFile(polhemus_male_file)
female_sheets = polhemus_female_xls.sheet_names
male_sheets = polhemus_male_xls.sheet_names

print("Available worksheets in PolhemusFemale.xlsx:", female_sheets)
print("Available worksheets in PolhemusMale.xlsx:", male_sheets)

# Function to find matching worksheet
def find_matching_sheet(subject, available_sheets):
    subject_lower = str(subject).lower()
    for sheet in available_sheets:
        sheet_lower = str(sheet).lower().replace(" ", "")
        if subject_lower in sheet_lower or sheet_lower.startswith(f"sub{subject_lower}") or sheet_lower == f"sub{subject_lower}fm" or sheet_lower == f"sub{subject_lower}mm":
            return sheet
    return None

for subject in subjects:
    # Determine which file to load based on the subject's sex
    if 'F' in subject:
        polhemus_file = polhemus_female_file
        available_sheets = female_sheets
    else:
        polhemus_file = polhemus_male_file
        available_sheets = male_sheets

    # Find the matching worksheet
    tab_name = find_matching_sheet(subject, available_sheets)

    if tab_name:
        try:
            polhemus_df = pd.read_excel(polhemus_file, sheet_name=tab_name)

            polhemus_df.columns = [
                'Sensor', 'X_position_cm', 'Y_position_cm', 'Z_position_cm',
                'Azimuth_attitude_degrees', 'Elevation_attitude_degrees', 'Roll_attitude_degrees'
            ]

            head_data = polhemus_df[polhemus_df['Sensor'] == 1]
            torso_data = polhemus_df[polhemus_df['Sensor'] == 2]

            summary = {
                'Subject': subject,
                'Head_X_Mean': head_data['X_position_cm'].mean(skipna=True),
                'Head_X_Std': head_data['X_position_cm'].std(skipna=True),
                'Head_Y_Mean': head_data['Y_position_cm'].mean(skipna=True),
                'Head_Y_Std': head_data['Y_position_cm'].std(skipna=True),
                'Head_Z_Mean': head_data['Z_position_cm'].mean(skipna=True),
                'Head_Z_Std': head_data['Z_position_cm'].std(skipna=True),
                'Torso_X_Mean': torso_data['X_position_cm'].mean(skipna=True),
                'Torso_X_Std': torso_data['X_position_cm'].std(skipna=True),
                'Torso_Y_Mean': torso_data['Y_position_cm'].mean(skipna=True),
                'Torso_Y_Std': torso_data['Y_position_cm'].std(skipna=True),
                'Torso_Z_Mean': torso_data['Z_position_cm'].mean(skipna=True),
                'Torso_Z_Std': torso_data['Z_position_cm'].std(skipna=True),
                'Azimuth_Mean': head_data['Azimuth_attitude_degrees'].mean(skipna=True),
                'Azimuth_Std': head_data['Azimuth_attitude_degrees'].std(skipna=True),
                'Elevation_Mean': head_data['Elevation_attitude_degrees'].mean(skipna=True),
                'Elevation_Std': head_data['Elevation_attitude_degrees'].std(skipna=True),
                'Roll_Mean': head_data['Roll_attitude_degrees'].mean(skipna=True),
                'Roll_Std': head_data['Roll_attitude_degrees'].std(skipna=True)
            }
            polhemus_summaries.append(summary)
        except Exception as e:
            print(f"Error processing Polhemus data for {subject}: {e}")
            summary = {
                'Subject': subject,
                'Head_X_Mean': np.nan, 'Head_X_Std': np.nan, 'Head_Y_Mean': np.nan, 'Head_Y_Std': np.nan,
                'Head_Z_Mean': np.nan, 'Head_Z_Std': np.nan, 'Torso_X_Mean': np.nan, 'Torso_X_Std': np.nan,
                'Torso_Y_Mean': np.nan, 'Torso_Y_Std': np.nan, 'Torso_Z_Mean': np.nan, 'Torso_Z_Std': np.nan,
                'Azimuth_Mean': np.nan, 'Azimuth_Std': np.nan, 'Elevation_Mean': np.nan, 'Elevation_Std': np.nan,
                'Roll_Mean': np.nan, 'Roll_Std': np.nan
            }
            polhemus_summaries.append(summary)
    else:
        print(f"No matching worksheet found for subject {subject}")
        summary = {
            'Subject': subject,
            'Head_X_Mean': np.nan, 'Head_X_Std': np.nan, 'Head_Y_Mean': np.nan, 'Head_Y_Std': np.nan,
            'Head_Z_Mean': np.nan, 'Head_Z_Std': np.nan, 'Torso_X_Mean': np.nan, 'Torso_X_Std': np.nan,
            'Torso_Y_Mean': np.nan, 'Torso_Y_Std': np.nan, 'Torso_Z_Mean': np.nan, 'Torso_Z_Std': np.nan,
            'Azimuth_Mean': np.nan, 'Azimuth_Std': np.nan, 'Elevation_Mean': np.nan, 'Elevation_Std': np.nan,
            'Roll_Mean': np.nan, 'Roll_Std': np.nan
        }
        polhemus_summaries.append(summary)

polhemus_summary_df = pd.DataFrame(polhemus_summaries)

# 1.4 Load Force Plate Data
force_plate_file = "/content/APAL 2019 Force Plate Data Combined.xlsx"
force_plate_combined_df = pd.read_excel(force_plate_file)

force_plate_summaries = []

for subject in subjects:
    for task in ['Inspection_Task', 'Search_Task']:
        subject_task_data = force_plate_combined_df[
            (force_plate_combined_df['Subject'] == subject) &
            (force_plate_combined_df['Task'] == task)
        ]

        if not subject_task_data.empty:
            summary = {
                'Subject': subject,
                f'Mediolateral_Mean_{task}': subject_task_data['Mediolateral'].mean(skipna=True),
                f'Mediolateral_Std_{task}': subject_task_data['Mediolateral'].std(skipna=True),
                f'Anterior_Posterior_Mean_{task}': subject_task_data['Anterior_Posterior'].mean(skipna=True),
                f'Anterior_Posterior_Std_{task}': subject_task_data['Anterior_Posterior'].std(skipna=True)
            }
        else:
            summary = {
                'Subject': subject,
                f'Mediolateral_Mean_{task}': np.nan,
                f'Mediolateral_Std_{task}': np.nan,
                f'Anterior_Posterior_Mean_{task}': np.nan,
                f'Anterior_Posterior_Std_{task}': np.nan
            }
        if task == 'Inspection_Task':
            subject_summary = summary
        else:
            subject_summary.update(summary)

    force_plate_summaries.append(subject_summary)

force_plate_summary_df = pd.DataFrame(force_plate_summaries)

# Step 2: Merge the Data
data = pd.merge(demographics_data, ssq_data, on='Subject', how='inner')
data = pd.merge(data, polhemus_summary_df, on='Subject', how='left')
data = pd.merge(data, force_plate_summary_df, on='Subject', how='left')

# Step 3: Handle Missing Data
# Impute missing kinematic data with the median (instead of zeros)
kinematic_columns = [
    'Head_X_Mean', 'Head_X_Std', 'Head_Y_Mean', 'Head_Y_Std', 'Head_Z_Mean', 'Head_Z_Std',
    'Torso_X_Mean', 'Torso_X_Std', 'Torso_Y_Mean', 'Torso_Y_Std', 'Torso_Z_Mean', 'Torso_Z_Std',
    'Azimuth_Mean', 'Azimuth_Std', 'Elevation_Mean', 'Elevation_Std', 'Roll_Mean', 'Roll_Std',
    'Mediolateral_Mean_Inspection_Task', 'Mediolateral_Std_Inspection_Task',
    'Anterior_Posterior_Mean_Inspection_Task', 'Anterior_Posterior_Std_Inspection_Task',
    'Mediolateral_Mean_Search_Task', 'Mediolateral_Std_Search_Task',
    'Anterior_Posterior_Mean_Search_Task', 'Anterior_Posterior_Std_Search_Task'
]
for col in kinematic_columns:
    if col in data.columns:
        data[col] = data[col].fillna(data[col].median())

# Impute missing demographic and SSQ data with median
numerical_columns = [
    'Age', 'Height_cm', 'Weight_lbs', 'Play_Video_Games', 'Age_Started_Games',
    'Years_Played_Games', 'Hours_Per_Week_Games', 'Used_VR_Headset', 'Owns_VR_Headset',
    'Hours_Per_Week_VR', 'SSQ1_Tc', 'SSQ2_Tc', 'Post_Exposure_Score'
]
for col in numerical_columns:
    if col in data.columns:
        data[col] = data[col].fillna(data[col].median())

# Step 4: Feature Engineering
le_sex = LabelEncoder()
le_driving = LabelEncoder()
data['Sex'] = le_sex.fit_transform(data['Sex'].astype(str))
data['Driving_Status'] = le_driving.fit_transform(data['Driving_Status'].astype(str))

# Create new features (differences between kinematic variables)
data['Head_Torso_X_Diff'] = data['Head_X_Mean'] - data['Torso_X_Mean']
data['Head_Torso_Y_Diff'] = data['Head_Y_Mean'] - data['Torso_Y_Mean']
data['Head_Torso_Z_Diff'] = data['Head_Z_Mean'] - data['Torso_Z_Mean']

# Select features and target
features = [
    'Sex', 'Driving_Status', 'Age', 'Height_cm', 'Weight_lbs', 'Play_Video_Games', 'Age_Started_Games',
    'Years_Played_Games', 'Hours_Per_Week_Games', 'Used_VR_Headset', 'Owns_VR_Headset', 'Hours_Per_Week_VR',
    'SSQ1_Tc', 'Head_X_Mean', 'Head_X_Std', 'Head_Y_Mean', 'Head_Y_Std', 'Head_Z_Mean', 'Head_Z_Std',
    'Torso_X_Mean', 'Torso_X_Std', 'Torso_Y_Mean', 'Torso_Y_Std', 'Torso_Z_Mean', 'Torso_Z_Std',
    'Azimuth_Mean', 'Azimuth_Std', 'Elevation_Mean', 'Elevation_Std', 'Roll_Mean', 'Roll_Std',
    'Mediolateral_Mean_Inspection_Task', 'Mediolateral_Std_Inspection_Task',
    'Anterior_Posterior_Mean_Inspection_Task', 'Anterior_Posterior_Std_Inspection_Task',
    'Mediolateral_Mean_Search_Task', 'Mediolateral_Std_Search_Task',
    'Anterior_Posterior_Mean_Search_Task', 'Anterior_Posterior_Std_Search_Task',
    'Head_Torso_X_Diff', 'Head_Torso_Y_Diff', 'Head_Torso_Z_Diff'
]
X = data[features]
y = data['Indicated_Sick_SSQ2']

# Step 5: Split the Data
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)

# Step 6: Scale the Features
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Step 7: Apply SMOTE
smote = SMOTE(random_state=42)
X_train_scaled, y_train = smote.fit_resample(X_train_scaled, y_train)

# Step 8: Define Models with Hyperparameter Tuning
# Random Forest
rf_param_grid = {
    'n_estimators': [100, 200, 300],
    'max_depth': [None, 10, 20],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 4]
}
rf = RandomForestClassifier(random_state=42, class_weight='balanced')
rf_grid = GridSearchCV(rf, rf_param_grid, cv=5, scoring='f1', n_jobs=-1)
rf_grid.fit(X_train_scaled, y_train)
best_rf = rf_grid.best_estimator_

# Linear SVM
svm_param_grid = {
    'estimator__C': [0.01, 0.1, 1, 10, 100]
}
linear_svc = LinearSVC(random_state=42, max_iter=10000, class_weight='balanced')
calibrated_svc = CalibratedClassifierCV(linear_svc, method='sigmoid', cv=5)
svm_grid = GridSearchCV(calibrated_svc, svm_param_grid, cv=5, scoring='f1', n_jobs=-1)
svm_grid.fit(X_train_scaled, y_train)
best_svm = svm_grid.best_estimator_

# AdaBoost
adaboost_param_grid = {
    'n_estimators': [50, 100, 200],
    'learning_rate': [0.01, 0.1, 1.0]
}
adaboost = AdaBoostClassifier(random_state=42)
adaboost_grid = GridSearchCV(adaboost, adaboost_param_grid, cv=5, scoring='f1', n_jobs=-1)
adaboost_grid.fit(X_train_scaled, y_train)
best_adaboost = adaboost_grid.best_estimator_

# XGBoost
xgb_param_grid = {
    'n_estimators': [100, 200, 300],
    'max_depth': [3, 5, 7],
    'learning_rate': [0.01, 0.1, 0.3],
    'subsample': [0.8, 1.0]
}
xgb = XGBClassifier(random_state=42, eval_metric='logloss', scale_pos_weight=len(y_train[y_train==0])/len(y_train[y_train==1]))
xgb_grid = GridSearchCV(xgb, xgb_param_grid, cv=5, scoring='f1', n_jobs=-1)
xgb_grid.fit(X_train_scaled, y_train)
best_xgb = xgb_grid.best_estimator_

# Logistic Regression
lr = LogisticRegression(random_state=42, class_weight='balanced')
lr.fit(X_train_scaled, y_train)

# Step 9: Stacking Ensemble
estimators = [
    ('rf', best_rf),
    ('svm', best_svm),
    ('adaboost', best_adaboost),
    ('xgb', best_xgb)
]
stacking = StackingClassifier(estimators=estimators, final_estimator=LogisticRegression())
stacking.fit(X_train_scaled, y_train)

# Step 10: Select Top Features
feature_importance = pd.DataFrame({
    'Feature': features,
    'Importance': best_rf.feature_importances_
}).sort_values(by='Importance', ascending=False)

top_features = feature_importance['Feature'].head(10).tolist()
print("Top 10 Features:", top_features)

X_train_top = pd.DataFrame(X_train_scaled, columns=features)[top_features]
X_test_top = pd.DataFrame(X_test_scaled, columns=features)[top_features]

# Step 11: Evaluate Models
models = {
    'Logistic Regression': lr,
    'Random Forest': best_rf,
    'Linear SVM': best_svm,
    'AdaBoost': best_adaboost,
    'XGBoost': best_xgb,
    'Stacking Ensemble': stacking
}

results = {}
for name, model in models.items():
    model.fit(X_train_top, y_train)

    y_pred = model.predict(X_test_top)
    y_pred_proba = model.predict_proba(X_test_top)[:, 1] if hasattr(model, "predict_proba") else None

    accuracy = accuracy_score(y_test, y_pred)
    precision = precision_score(y_test, y_pred, zero_division=0)
    recall = recall_score(y_test, y_pred, zero_division=0)
    f1 = f1_score(y_test, y_pred, zero_division=0)
    roc_auc = roc_auc_score(y_test, y_pred_proba) if y_pred_proba is not None else None

    cv_scores = cross_val_score(model, X_train_top, y_train, cv=5, scoring='f1')

    results[name] = {
        'Accuracy': accuracy,
        'Precision': precision,
        'Recall': recall,
        'F1-Score': f1,
        'ROC-AUC': roc_auc,
        'CV F1 Mean': cv_scores.mean(),
        'CV F1 Std': cv_scores.std()
    }

    print(f"\nClassification Report for {name}:")
    print(classification_report(y_test, y_pred, zero_division=0))

# Step 12: Display Results
results_df = pd.DataFrame(results).T
print("\nModel Comparison:")
print(results_df)

# Step 13: Visualize Feature Importance
feature_importance_top = pd.DataFrame({
    'Feature': top_features,
    'Importance': best_rf.feature_importances_
}).sort_values(by='Importance', ascending=False)

plt.figure(figsize=(10, 6))
sns.barplot(x='Importance', y='Feature', data=feature_importance_top)
plt.title('Feature Importance (Random Forest - Top Features)')
plt.show()
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt


polhemus_file = "/content/PolhemusFemale.xlsx"
subject_id = "3F"  # Replace with the desired subject ID (e.g., '3F', '1M')

# Load the Excel file and get the sheet names
polhemus_xls = pd.ExcelFile(polhemus_file)
sheet_names = polhemus_xls.sheet_names
print("Available worksheets:", sheet_names)

# Function to find the matching worksheet for the subject
def find_matching_sheet(subject, available_sheets):
    subject_lower = str(subject).lower()
    for sheet in available_sheets:
        sheet_lower = str(sheet).lower().replace(" ", "")
        if subject_lower in sheet_lower or sheet_lower.startswith(f"sub{subject_lower}") or sheet_lower == f"sub{subject_lower}fm" or sheet_lower == f"sub{subject_lower}mm":
            return sheet
    return None

# Find the worksheet for the subject
tab_name = find_matching_sheet(subject_id, sheet_names)
if not tab_name:
    raise ValueError(f"No matching worksheet found for subject {subject_id}")

# Load the data for the subject
polhemus_df = pd.read_excel(polhemus_file, sheet_name=tab_name)
polhemus_df.columns = [
    'Sensor', 'X_position_cm', 'Y_position_cm', 'Z_position_cm',
    'Azimuth_attitude_degrees', 'Elevation_attitude_degrees', 'Roll_attitude_degrees'
]

# Step 3: Process the data for head (Sensor 1)
head_data = polhemus_df[polhemus_df['Sensor'] == 1].copy()

# Assume a sampling rate (e.g., 10 Hz, meaning 0.1 seconds per sample)
# Adjust this based on your actual data's sampling rate
sampling_rate = 10  # Hz
time_step = 1 / sampling_rate  # seconds
head_data['Time'] = np.arange(len(head_data)) * time_step

# Step 4: Calculate derivatives for linear motion (X position)
# Position (already in cm)
x_position = head_data['X_position_cm'].values

# Velocity (cm/s) = Δposition / Δtime
velocity = np.gradient(x_position, head_data['Time'])  # First derivative

# Acceleration (cm/s²) = Δvelocity / Δtime
acceleration = np.gradient(velocity, head_data['Time'])  # Second derivative

# Jerk (cm/s³) = Δacceleration / Δtime
jerk = np.gradient(acceleration, head_data['Time'])  # Third derivative

# Step 5: Calculate derivatives for angular motion (Yaw)
# Yaw (in degrees)
yaw = head_data['Azimuth_attitude_degrees'].values

# Angular velocity (degrees/s) = Δyaw / Δtime
angular_velocity = np.gradient(yaw, head_data['Time'])

# Angular acceleration (degrees/s²) = Δangular_velocity / Δtime
angular_acceleration = np.gradient(angular_velocity, head_data['Time'])

# Angular jerk (degrees/s³) = Δangular_acceleration / Δtime
angular_jerk = np.gradient(angular_acceleration, head_data['Time'])

# Step 6: Create the visualizations
fig, axes = plt.subplots(4, 2, figsize=(12, 10), sharex=True)

# Panel (a): Linear Motion
# X Position
axes[0, 0].plot(head_data['Time'], x_position, 'k-', label='X position (cm)')
axes[0, 0].set_ylabel('X position (cm)')
axes[0, 0].legend(loc='upper right')

# Velocity
axes[1, 0].plot(head_data['Time'], velocity, 'b-', label='Velocity (cm/s)')
axes[1, 0].set_ylabel('Velocity (cm/s)')
axes[1, 0].legend(loc='upper right')

# Acceleration
axes[2, 0].plot(head_data['Time'], acceleration, 'g-', label='Acceleration (cm/s²)')
axes[2, 0].set_ylabel('Acceleration (cm/s²)')
axes[2, 0].legend(loc='upper right')

# Jerk
axes[3, 0].plot(head_data['Time'], jerk, 'c-', label='Jerk (cm/s³)')
axes[3, 0].set_ylabel('Jerk (cm/s³)')
axes[3, 0].set_xlabel('Time (seconds)')
axes[3, 0].legend(loc='upper right')
axes[3, 0].text(-0.1, 1.1, '(a)', transform=axes[3, 0].transAxes, fontsize=12, fontweight='bold', va='top')

# Panel (b): Angular Motion
# Yaw
axes[0, 1].plot(head_data['Time'], yaw, 'k-', label='Yaw (degrees)')
axes[0, 1].set_ylabel('Yaw (degrees)')
axes[0, 1].legend(loc='upper right')

# Angular Velocity
axes[1, 1].plot(head_data['Time'], angular_velocity, 'b-', label='Velocity (degrees/s)')
axes[1, 1].set_ylabel('Velocity (degrees/s)')
axes[1, 1].legend(loc='upper right')

# Angular Acceleration
axes[2, 1].plot(head_data['Time'], angular_acceleration, 'g-', label='Acceleration (degrees/s²)')
axes[2, 1].set_ylabel('Acceleration (degrees/s²)')
axes[2, 1].legend(loc='upper right')

# Angular Jerk
axes[3, 1].plot(head_data['Time'], angular_jerk, 'c-', label='Jerk (degrees/s³)')
axes[3, 1].set_ylabel('Jerk (degrees/s³)')
axes[3, 1].set_xlabel('Time (seconds)')
axes[3, 1].legend(loc='upper right')
axes[3, 1].text(-0.1, 1.1, '(b)', transform=axes[3, 1].transAxes, fontsize=12, fontweight='bold', va='top')

# Adjust layout
plt.tight_layout()
plt.show()
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import confusion_matrix, roc_curve, auc
from google.colab import files

# Assuming the following variables are already defined from your previous code:
# - data: Merged DataFrame with all features and target ('Indicated_Sick_SSQ2')
# - X_test_top: Scaled test data with top features
# - y_test: True labels for the test set
# - Models: lr, best_rf, best_svm, best_adaboost, best_xgb, stacking
# - polhemus_female_file, polhemus_male_file: Paths to Polhemus data files
# - subjects: List of subject IDs
# - top_features: List of top 10 features from Random Forest

# Step 1: Data Exploration Visualizations

# 1.1 Distribution of SSQ Scores by Sickness Status
plt.figure(figsize=(10, 5))
plt.subplot(1, 2, 1)
sns.histplot(data=data, x='SSQ1_Tc', hue='Indicated_Sick_SSQ2', multiple='stack', bins=20)
plt.title('Distribution of SSQ1_Tc by Sickness Status')
plt.xlabel('SSQ1_Tc (Pre-Exposure)')
plt.ylabel('Count')

plt.subplot(1, 2, 2)
sns.histplot(data=data, x='SSQ2_Tc', hue='Indicated_Sick_SSQ2', multiple='stack', bins=20)
plt.title('Distribution of SSQ2_Tc by Sickness Status')
plt.xlabel('SSQ2_Tc (Post-Exposure)')
plt.ylabel('Count')
plt.tight_layout()
plt.show()

# 1.2 Correlation Heatmap of Features
plt.figure(figsize=(12, 10))
correlation_matrix = data[['SSQ1_Tc', 'SSQ2_Tc', 'Age', 'Height_cm', 'Weight_lbs', 'Hours_Per_Week_VR'] + top_features].corr()
sns.heatmap(correlation_matrix, annot=False, cmap='coolwarm', vmin=-1, vmax=1)
plt.title('Correlation Heatmap of Features')
plt.show()

# Step 2: Feature Analysis Visualizations

# 2.1 Boxplots of Key Kinematic Features by Sickness Status
plt.figure(figsize=(12, 8))
for i, feature in enumerate(top_features[:4], 1):  # Plot first 4 top features
    plt.subplot(2, 2, i)
    sns.boxplot(x='Indicated_Sick_SSQ2', y=feature, data=data)
    plt.title(f'{feature} by Sickness Status')
plt.tight_layout()
plt.show()

# 2.2 Scatter Plot of SSQ2_Tc vs. a Kinematic Feature
plt.figure(figsize=(8, 6))
sns.scatterplot(x='SSQ2_Tc', y=top_features[0], hue='Indicated_Sick_SSQ2', size='Indicated_Sick_SSQ2', data=data)
plt.title(f'SSQ2_Tc vs. {top_features[0]}')
plt.xlabel('SSQ2_Tc (Post-Exposure)')
plt.ylabel(top_features[0])
plt.show()

# Step 3: Model Performance Visualizations

# 3.1 Confusion Matrix for the Best Model (AdaBoost)
y_pred_adaboost = best_adaboost.predict(X_test_top)
cm = confusion_matrix(y_test, y_pred_adaboost)
plt.figure(figsize=(6, 5))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False,
            xticklabels=['Not Sick (0)', 'Sick (1)'], yticklabels=['Not Sick (0)', 'Sick (1)'])
plt.title('Confusion Matrix for AdaBoost')
plt.xlabel('Predicted')
plt.ylabel('True')
plt.show()

# 3.2 ROC Curves for All Models
plt.figure(figsize=(10, 8))
models = {
    'Logistic Regression': lr,
    'Random Forest': best_rf,
    'Linear SVM': best_svm,
    'AdaBoost': best_adaboost,
    'XGBoost': best_xgb,
    'Stacking Ensemble': stacking
}

for name, model in models.items():
    if hasattr(model, "predict_proba"):
        y_pred_proba = model.predict_proba(X_test_top)[:, 1]
        fpr, tpr, _ = roc_curve(y_test, y_pred_proba)
        roc_auc = auc(fpr, tpr)
        plt.plot(fpr, tpr, label=f'{name} (AUC = {roc_auc:.2f})')

plt.plot([0, 1], [0, 1], 'k--', label='Random Guess')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC Curves for All Models')
plt.legend(loc='lower right')
plt.show()

# Step 4: Kinematic Patterns Visualization

# 4.1 Mean X Position Trajectories for Sick vs. Non-Sick Subjects
# Collect X position data for all subjects, separated by sickness status
sick_subjects = data[data['Indicated_Sick_SSQ2'] == 1]['Subject'].tolist()
not_sick_subjects = data[data['Indicated_Sick_SSQ2'] == 0]['Subject'].tolist()

# Initialize lists to store trajectories
sick_trajectories = []
not_sick_trajectories = []

# Sampling rate (adjust based on your data)
sampling_rate = 10  # Hz
time_step = 1 / sampling_rate

# Process Polhemus data for each subject
for subject in subjects:
    # Determine which file to load
    if 'F' in subject:
        polhemus_file = polhemus_female_file
        available_sheets = pd.ExcelFile(polhemus_female_file).sheet_names
    else:
        polhemus_file = polhemus_male_file
        available_sheets = pd.ExcelFile(polhemus_male_file).sheet_names

    # Find the matching worksheet
    tab_name = find_matching_sheet(subject, available_sheets)
    if not tab_name:
        continue

    # Load the data
    try:
        polhemus_df = pd.read_excel(polhemus_file, sheet_name=tab_name)
        polhemus_df.columns = [
            'Sensor', 'X_position_cm', 'Y_position_cm', 'Z_position_cm',
            'Azimuth_attitude_degrees', 'Elevation_attitude_degrees', 'Roll_attitude_degrees'
        ]
        head_data = polhemus_df[polhemus_df['Sensor'] == 1].copy()
        head_data['Time'] = np.arange(len(head_data)) * time_step
        x_position = head_data['X_position_cm'].values

        # Truncate or pad the trajectory to a fixed length (e.g., 4000 samples)
        fixed_length = 4000
        if len(x_position) > fixed_length:
            x_position = x_position[:fixed_length]
        else:
            x_position = np.pad(x_position, (0, fixed_length - len(x_position)), mode='constant', constant_values=np.nan)

        # Add to the appropriate list
        if subject in sick_subjects:
            sick_trajectories.append(x_position)
        else:
            not_sick_trajectories.append(x_position)
    except Exception as e:
        print(f"Error processing Polhemus data for {subject}: {e}")
        continue

# Calculate mean trajectories
sick_mean = np.nanmean(sick_trajectories, axis=0)
not_sick_mean = np.nanmean(not_sick_trajectories, axis=0)
time = np.arange(fixed_length) * time_step

# Plot the mean trajectories
plt.figure(figsize=(10, 6))
plt.plot(time, sick_mean, 'r-', label='Sick (Mean)')
plt.plot(time, not_sick_mean, 'b-', label='Not Sick (Mean)')
plt.xlabel('Time (seconds)')
plt.ylabel('X Position (cm)')
plt.title('Mean X Position Trajectories: Sick vs. Not Sick')
plt.legend()
plt.show()